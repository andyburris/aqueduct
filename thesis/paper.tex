\documentclass[manuscript,review,anonymous]{acmart}
% note: Overleaf includes acmart.cls for you so you don't even need that
% it even includes the ACM-ReferenceFormat.* files, but I edited the .bst file to not warn about missing publisher/address for inproceedings

\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
%\acmDOI{10.1145/1122445.1122456}

\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{booktabs} % use booktabs instead of ugly regular tables
\usepackage{graphicx} % more figure options
\RequirePackage[l2tabu, orthodox]{nag} % checks for common LaTeX errors
\usepackage{microtype} % better typesetting
\usepackage[utf8]{inputenc} % lenient to utf-8 characters like smart quotes
\usepackage{refcheck} % warns about unreferenced figures/tables that have labels. remove it before submitting
\usepackage[subtle]{savetrees} % denser formatting, you can comment this out

\interfootnotelinepenalty=10000 % split footnotes are ugly
\tolerance=400 % reduce how often words stick out into columns at the expense of word spacing

\graphicspath{{figures/}} % put all your figures in this folder

\begin{document}

\title{Aqueduct: Transforming Snapshots of Data into Event Streams}

%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Andrew Burris}
\affiliation{%
  \institution{Brown University}
  \country{United States}
}
\email{andrew_burris@brown.edu}

\renewcommand{\shortauthors}{Lastname, et al.}

\begin{abstract}
  Aqueduct is a library that takes snapshot-style methods data access (like web APIs and data exports) and turns them into streams of events. 
  It periodically (but efficiently) polls a new snapshot, then reconciles it with the previous stored snapshot, and calculates the events from the difference between the two. 
  After investigating why API access and right to data portability (RtDP) regulations haven't made meaningful differences in data interoperability, it became clear that one-time snapshots of data were insufficient for modern applications. 
  With Aqueduct, and the ecosystem of extensible pre-built integrations that will hopefully grow around it, developers will be able to integrate cloud data into their applications in a richer and easier way.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
     <concept>
         <concept_id>10011007.10011006.10011072</concept_id>
         <concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10011007.10011006.10011071</concept_id>
         <concept_desc>Software and its engineering~Software configuration management and version control systems</concept_desc>
         <concept_significance>100</concept_significance>
         </concept>
     <concept>
         <concept_id>10011007.10011006.10011066</concept_id>
         <concept_desc>Software and its engineering~Development frameworks and environments</concept_desc>
         <concept_significance>300</concept_significance>
         </concept>
   </ccs2012>
\end{CCSXML}
  
\ccsdesc[500]{Software and its engineering~Software libraries and repositories}
\ccsdesc[100]{Software and its engineering~Software configuration management and version control systems}
\ccsdesc[300]{Software and its engineering~Development frameworks and environments}

\keywords{web APIs, data exports, data portability, data interoperability, event streams, data integration}

\maketitle

\section{Introduction}

In the early days of computing, some of the first software developed was business-critical applications: calendar, email, word processing, etc. Looking at those applications today, they share a characteristic--the concept of the "client". Users can interact with their email interoperably from any and all of the iOS Mail app, the Gmail website, or a specialized email client for people with visual impairment, to name a few examples. The same goes for calendars, and even word processing: \texttt{.docx} files can be opened in Microsoft Word, but also LibreOffice, Google Docs, and many more.

The 2000s brought a new paradigm: cloud computing. Cloud applications have clear benefits: the ability to access them from any device, multiplayer functionality, the reliability of data being constantly backed up off-site, and more. But in exchange for the convenience of having data in the cloud, users gave up the option to switch between clients. Because cloud data is stored on a company's servers, users have to use that company's client app to access it.

As an example, compare the older technology of MP3 clients with one of their modern cloud counterparts: Spotify. If a user wanted to switch between traditional MP3 clients, all they had to do was install a new one, point it at the same folder containing all their \texttt{.mp3} music files, and it would work. Users could even use multiple at the same time. On the other hand, users constantly complain about changes Spotify makes to their app, but can't do anything about it. Switching to an alternative music streaming service requires hand-migrating all of their data (playlists, saved songs, etc.), not to mention it simply puts them at the whims of a different company's client app.

Aqueduct's goal is to enable developers to interact with 3rd party data so easily that it becomes possible to make client-style software for the new generation of cloud apps.

\section{Current Solutions}
It isn't the case that there's no way to access cloud data. Many cloud apps allow 3rd party developers to access their data through application programming interfaces, abbreviated as "APIs". Even cloud apps that don't offer API access usually give users ways to export data from them (if only because they're required to for regulatory reasons).~\cite{kramerMakingDataPortability}

However, these APIs are fundamentally very challenging to make clients with. In order to get data from an API, a developer has to have their app request a webpage containing machine-readable text from the cloud server that contains a snapshot of the cloud data at that time. Often, each snapshot only includes a subset of the overall data, so multiple requests need to be stitched together to get the full picture.~\cite{Mainframe} (To further complicate the process, if something updates in between those stitched requests, those discrepancies need to be handled.)

This request-each-snapshot model isn't really how the official clients work.~\cite{spotifyengineeringSpotifyModernizesClientSide2020} Most have a constant two-way connection open with the backend server, and after an initial download of a full copy of the data, any changes from either side are sent as a notification about the specific change. The benefit is both that the notifications are real-time, and that they're specific, which makes them much more efficient on resources like network and database bandwidth. Since traditional API access lacks this event-stream style of communication, it's too unwieldy to easily create clients with. This isn't an intentional barrier; they're just not suited for modern application design.

One solution would be to run these snapshots through a distributed version control system, such as Git. There are a few downsides to this, though. One is that version control systems are usually not automatic, so would require manual interventions from users at times. Additionally, they mainly only handle finding differences between text--more vague pieces of data "moving around" is outside of their scope, as is any of the logic for actually getting that data.

Another solution that has been proposed is to change the cloud paradigm entirely. The local-first movement is the biggest player in this space: local-first apps store data primarily on-device rather than on-server, using conflict-resolution algorithms to automatically (and optionally) sync changes to data between different devices.~\cite{kleppmannLocalfirstSoftwareYou2019} While there have been notable success stories, like Obsidian and Linear, the grander vision of the local-first movement is one where all of a user's data is local-first, and thus accessible to them. The issue is that in order to get someone to use, say, a maps application that allows users to store their personal data local-first, it first needs to be comparable in quality to the Google or Apple Maps app they're already using.~\cite{syrmoudisDataPortabilityOnline2021} (As Apple's decade-long journey to compete with Google Maps shows, that's not a trivial task. Also, it's impractical--personal data like a user's location history is very important, but only a fraction of what Google Maps does.) In order for a local-first ecosystem to thrive, it needs to be able to interact with data from the cloud applications people already use.

Finally, there is the concept of API "integrations" (also often called SDKs)--libraries that provide helper code that makes it easier for developers to interact with data from other apps' APIs in their own apps. Most of these are written by the cloud companies that provide the APIs, and have all of the limitations described above. There are a number of libraries that attempt to standardize API integrations to give developers a unified interface for accessing them.~\cite{SurferOrgProtocol2025}~\cite{Mainframe}~\cite{Nango} Some are just a standardization of traditional snapshot-style requests, but a few sync web API data with a 3rd-party developer's database, creating a much more reactive interface.~\cite{Sequin} However, using a standardized integration library, even an open source one, locks developers into the APIs that it supports. If a developer needs to interact with a more bespoke service that isn't in the library (or like Aqueduct, wants to reconcile with other sources like data exports), they either have to build it themselves but outside of the standardized ecosystem, or they have to get in contact with the maintainers of the library and try to get them to write the new integration. 

\section{Related Work}
Much research has been done around the concept of data portability in the cloud, which is very helpful in elucidating the ways cloud apps expose data. Krämer et al. has a comprehensive listing of the ways that cloud apps have made their data available under the EU's directive of "right to data portability" (RtDP). One helpful differentiation is the idea of relatively "synchronous" access, like APIs, and relatively "asynchronous" access, like data exports that take weeks to compile. It also covers why projects aiming to change the siloed cloud paradigm, like SOLID, haven't been effective, partially because they have no interoperability with existing cloud providers.~\cite{kramerMakingDataPortability}

A number of studies have explored users' experience with RtDP, mostly finding that it has not had the desired impact due to its unintuitive nature. Kuebler-Wachendorff et al. finds that users don't know about RtDP or what it could be used for, and thus don't feel comfortable with it.~\cite{kuebler-wachendorffRightDataPortability2021} Luzsa et al. finds that only technologically adept people have been using RtDP to achieve "digital sovereignty".~\cite{luzsaOnlineServiceSwitching2022} Syrmoudis et al., 2021 provides sources for a lot of the regulations that establish RtDP, and notes that they require "interoperability", which essentially hasn't been seen so far. It also questions why current applications largely do not take advantage of being able to import from their competitors.~\cite{syrmoudisDataPortabilityOnline2021} This is partially explained in Syrmoudis et al., 2024 which finds that users who do take advantage of RtDP use it to transfer data between existing accounts rather than create new ones, showing that the use case is mainly this "interoperability".~\cite{syrmoudisUnlockingPersonalData2024}

There is a lot of literature about the relationship between snapshots and events in the domain of event sourcing, but this is in the opposite direction of my research. Event sourcing explores reconstructing a snapshot from a series of events, whereas Aqueduct tries to reconstruct those events from snapshots. However, Overeem et al. describes the challenges of changing schema within event-based systems in general, which is relevant because Aqueduct deals with multiple schema inherently, all of which might change on a whim.~\cite{overeemDarkSideEvent2017}

Some, but much less research has been done in the snapshot-to-events direction, especially because Aqueduct considers the surrounding components of this translation as well, such as reactivity, and reconciling and filling in data with different information from multiple sources. Cao et al. has many insights on converting snapshots of JSON data into events that track specifically what has changed about the data, which is very helpful in the specific diffing process.~\cite{caoJSONPatchTurning2016} Vesdapunt and Garcia-Molina explores a theoretical method of getting periodically updated data from an API with a minimally efficient number of actual calls to that API, which is quite relevant for the task of filling in data efficiently.~\cite{vesdapuntUpdatingExistingSocial2016}

\subsection{Related Area 2}
Another related area.
Same idea.

Example citation~\cite{wallace2017drafty}. If you're really ambitious, use autoref (requires hyperref package). Note the tilde shown in the source is a non-breaking space. It's a space, so you don't need to put an additional space before it.

Some reviewers don't like it if you use citations as part of a sentence, like saying that~\cite{wallace2017drafty} is a relevant paper. Rather, they want the author name(s) there as well, like Wallace et al.~\cite{wallace2017drafty} is a relevant paper.

\section{Research Questions}
Setting out with a goal of creating a library to make it easier to interact with cloud data, I decided on three central research questions to define what "making it easier" actually looks like:
\begin{enumerate}
  \item What are the ways apps expose data? What are the barriers to accessing it?
  \item What does a great ecosystem of integrations look like? How should data be represented and stored?
  \item Based on the ”inputs” from Q1 and the ”outputs” from Q2, what should the process to transform the former to the latter? (Those transformations being the integrations themselves.) What building blocks are most useful to help developers build these integrations?
\end{enumerate}

\section{The limitations of snapshot-style APIs}
The first part of my research consisted of exploring my first research question: "What are the ways apps expose data?" It was in this process that I started recognizing the concept of (and limitations of) the "snapshot-style" API. After reviewing the data access options for dozens of cloud applications,~\cite{kramerMakingDataPortability} I began to group them into five general categories (though there were, of course, outliers):
\begin{enumerate}
  \item **OAuth** (or similar login-based API) \\
  Most large cloud apps (Google, Spotify, etc.) have some kind of official login-based API, almost all using a protocol called OAuth. Often they have several different OAuth-type flows to authenticate different clients based on what platform they’re on. Websites, unlike servers, can be reliably identified due to the domain name the request originates from. But servers can store things securely, whereas any data in a web browser is user-accessible and thus insecure. This leads to different authentication flows based on what information can be shared.
  \item **API Key** \\
  Some simpler or developer-focused apps offer users API keys for 3rd-party access. (This also occurs in unofficial APIs, like Notion, where the initial data download on a webpage is made through traditional API requests; it's just not documented and the user has to get their API key from their cookies.). This method is usually the easiest for the developer but it can be complex and confusing for the user to obtain their key.
  \item **Export** \\
  Both large and small apps offer full data export options, though some (e.g. Google) have more complete information than others (e.g. Notion). These data exports are also often complex for the user to obtain. One benefit, however, is that once the file is obtained, it requires no authentication to parse it. (Though if the export has incomplete data, it's often necessary to use that app's API to fill out the full data.)
  \item **Scraping** \\
  Sometimes, a website offers no official way to obtain data, so it must be scraped from the web page that displays it. This is the hardest for developers to get right, because the website's structure can be changed on a whim, breaking the scraping code until the developer updates it. Thus, any solution involving scraping needs to be flexible, and should be a last resort.
  \item **Platform-specific** \\
  Some data is easily attainable if the integration is running on a specific platform, but complex otherwise. For example:
  \begin{itemize}
    \item An integration in an Android app can easily ask for permission and access text messages, which would otherwise require a complex exporting process
    \item An integration running on a Mac can easily access Apple Reminders and Notes through AppleScript, which would otherwise require very complex scraping from the iCloud website.
  \end{itemize}
\end{enumerate}   

Notably, all of these access methods are one-time snapshots of data. They quickly stop being up-to-date (and in fact, exports can often take weeks to be created, making them inherently outdated). 

Another major friction point I found with these current APIs was authentication. One issue is that there are so many options, even for a single app's data. Between the different authentication methods per platform, the different scopes of data being requested, and the fact that every app's API handles authentication slightly differently, it can be a long time before developers figure out how to properly request data, before even beginning to build anything on top of that.

In addition to the plethora of methods of authentication, there's a sequencing problem. A typical API interaction goes like this: 
\begin{enumerate}
  \item The app tries to request data from an API.
  \item If it returns successfully, great. If not, the app next checks if the authentication token needs to be refreshed before retrying.
  \item If that wasn't the problem, it then checks whether the user has logged in at all, and redirects them to do that if not.
\end{enumerate}
The issue is that most apps that interact with API data do so in many places, often on pages far away from the one where the user is prompted to log in. That means that any time a developer wants to write code that accesses API data at some place in their application, they also have to write all of this checking and redirection logic. (In reality, a lot of apps assume the user has logged in in one location and if it doesn't work, they reset the entire transaction and return to the home page to have the user log back in.)

To recap, this line of questioning led me to a few major issues with the current methods of interactions with APIs that Aqueduct needed to address. First, the snapshot problem: it's brittle to work with one-time captures of data. Second, the resource problem (an offshoot of the snapshot problem): it takes considerable resources to fetch and save all of the data needed from a web API each time it needs to be updated. Third, the authentication problem: it's hard for developers to even get started with accessing data due to complex and diverse authentication methods.

\section{Towards better integrations}
The second part of my research involved thinking through what a great developer experience for developers to integrate with 3rd party data would look like. One goal from the start was to make the ecosystem extensible by anyone. Other projects that brought together lots of integrations weren't easily extensible, which meant that when inevitably a developer needed an integration with an unsupported application, it became much more difficult to keep using that project. So I began by thinking about how to make it easy to build new integrations.

\subsection{Building integrations}
In order to make building integrations easier, a big factor was making them composable. Historically, composability has been extremely beneficial for building out robust developer ecosystems, and that applies here equally well.~\cite{piszekComposabilityOnlyGame2021} Consider a developer wanting to build an integration that uses data from Google Takeout. Currently, they must first build an integration with Google Drive, which is where the Takeouts are exported to (or Gmail, or Microsoft OneDrive, or a few other platforms). Only once they've done all of the work to authenticate with Drive, check it for new Takeout \texttt{.zip} files, and download those \texttt{.zip} files can they begin to work on the actual parsing of said files. 

Instead, imagine they could compose their integration on top of an existing Google Drive integration, where another developer had already handled all of those steps. All the current developer would have to do would be to consume the \texttt{.zip} file output by the Drive integration and parse the Takeout data--no authentication handling, no refresh handling, just a simple function mapping one type of data to another. 

One key aspect of this ecosystem is that integrations have to be open source packages in order to be built on top of. How open-source should they have to be? I didn't want to preclude developers from building integrations that were scoped locally to their private projects. On the other hand, integrations can't be built on if they're private. Open source is also important for transparency: if code is running on users' personal data, anyone should be able to inspect that it's not doing anything malicious. This is still an ongoing question for me, but the general goal is to maximize developer freedom so long as a healthy ecosystem is maintained.

Another overarching goal was to make it possible for these integrations to be flexible towards their disparate data sources. A single integration might take in different pieces of data from a virtually instantaneous web API, a data export that takes a month to compile, a web scraping utility that might fail when a website changes their layout, or a database accessible only on a specific operating system. All of these sources might have differing levels of richness, and data from one source might often overlap with data from another source but in a different format. Some sources might be less reliable, and fail often, or deliver out of date data (especially with exports). These all would need to be reconciled into a single source of truth in order to effectively create an API that could be relied upon. Moreover, each integration has a different landscape of sources, so Aqueduct needed to be flexible in defining this reconciliation process per-integration.

\subsection{Using integrations}
Most of my early work on Aqueduct revolved around trying to define a unified interface for developers to build integrations on top of, generalizing the methods of authentication and data access I'd defined into a cohesive system. While I had good explorations, I made very little progress in terms of actually implementing the system. I had been so focused on making it easy for developers to create integrations that I'd forgotten all about the developers actually *using* the integrations. No matter how easy it is to build an integration with Aqueduct, if developers don't then want to use that integration, it won't matter. And if they aren't going to be used, no one will want to build these integrations in the first place.

So I started focusing on a new question: "What does a significantly better developer experience (DX) for integrating with 3rd party data look like?" It was in grappling with this question that the "snapshot problem" finally clicked--real clients are working with streams of events, not one-off requests. This feels obvious in retrospect but it really wasn't from the start. I began to conceptualize a new kind of API in the form of an event stream. Each step in the chain of composition wouldn't just run once, once the past value had completed; it would listen continuously for the previous step to emit values at any time, re-run on each of those, and emit values of its own. (These kinds of streams aren't new--they exist as primitives or in libraries for languages like Kotlin, Java, Rust, and in fact JavaScript--but they have almost never been used for calling APIs, a task seemingly antithetical to the continuous concept.)  Furthermore, the stream wouldn't have to output the entirety of the data each time. Instead, it could output just the parts of the data that had changed from the previous snapshot, emulating as closely as possible the event-driven architecture of most real clients.

With the end goal principles in mind, the design of more specific features began to fall into place. Before covering those design considerations, another recap. I ended up with a few major goals for what using Aqueduct should entail. First, it should be easy to build new integrations in a composable manner, ideally as part of an open-source ecosystem. Second, it needs to be able to reconcile differences in data richness, timing, and even reliability of success. Third, it should have great DX, so developers will actually build and use these integrations.

\section{Architecture}
At a high level, the goal of Aqueduct is to make it easy to transform a snapshot-style API into an event stream-style API. Those event stream APIs need to be much better to use than the traditional ones, so developers will actually adopt them. And since no one developer can build every possible integration, it needs to be easy for anyone to build an integration and (ideally) contribute it to the ecosystem.

To turn snapshots into reactive event streams, Aqueduct has two core primitives: the \texttt{Stream} class and the \texttt{fetchReconcileDiff} function. 

\texttt{Stream}, as one might expect, is responsible for constructing the reactive stream. \texttt{fetchReconcileDiff} is responsible for turning snapshots into a set of events, in a way that is as resource-efficient as possible. Together they combine to solve the entire transformation. (There are other helper functions that provide further help on top of \texttt{Stream} and \texttt{fetchReconcileDiff} for common use cases, but no matter what, almost all integrations have their foundations in this two-pronged framework.)

\subsection{\texttt{Stream}'}
\texttt{Stream} chains together a sequence of functions, each one of which runs every time the previous one emits a value. If the previous function hasn't emitted a value yet, the next function won't run. Each step of the \texttt{Stream} can be one of many helper functions, which might transform data, handle asynchronous requests, or repeat a request every \texttt{n} seconds,  to name a few. 

\texttt{Stream} was originally conceived as a solution to the "authentication problem", where each data request relied on a trust that the current authentication state was valid, and required complex series of logic to deal with the times it inevitably wasn't. \texttt{Stream} inverts this: instead of cascading down to authentication checks when a data request fails, the data requests only run once the authentication checks have succeeded. 

This is especially useful when there are multiple steps to authenticate with a service. Consider the authentication flow for the Google Drive API, which begins with a verification code returned from the authorization screen on the Google Drive website. That verification code is then exchanged with the Google Drive server for an access token, which must be attached to every data request. To make matters worse, that access token will eventually expire, and must be regenerated using the refresh token that was also returned from the server, so the refresh check has to be run before making any data request.~\cite{GoogleapinodejsclientREADMEmd}

It is immensely complicated to keep track of which prerequisites have been fulfilled, which is what \texttt{Stream} solves. With it, this mess of authentication becomes much easier to follow. Any new code value gets exchanged for an access token, each token value is periodically validated and refreshed if needed, and every validated token value can have data requests run on it. If the token becomes invalid at some point, the data requests simply stop until it's back.

This allows developers using these integrations to write much simpler code. The code that handles requesting data only has to handle storing data, instead of also handling checking and storing authentication, and vice versa. 

It also imposes constraints on authentication when building the integrations, which counterintuitively make that process significantly simpler. Because each step of the chain can run an arbitrary amount of times, the function for each step must be idempotent (have no side effects). This is unusual. Many authentication flows involve setting credentials once and assuming they'll still be there in the next steps. But by having each step depend only its inputs, it removes the ability for stored credentials to silently get out of date and cause bugs. It also makes it clear what data is being used for which purposes, increasing the developer using it's understanding of how the authentication flow actually works.

With how simple these idempotent functions made authentication, the natural next step was to extend it to the other parts of the sync process. (This took a long time to figure out, but it was the breakthrough that finally solved how to make a legitimately better DX for developers using these integrations.) Each data request function is similarly idempotent, depending only on the prerequisites it takes as inputs, such as authentication tokens, previous results, and search parameters. This again has the benefit of making it very clear what information is needed to make these requests--no subtle bugs from mistakenly modified global state.

Another benefit of the \texttt{Stream} design is that it enables composability in a simple way. Instead of an integration having to consider where it gets its inputs from, it just has to take in any \texttt{Stream} of its inputs. Returning to the Google Takeout example from above, that integration's developer doesn't have to worry if it's getting its files from Google Drive or Gmail or OneDrive or a simple file upload. The Takeout integration simply needs to accept \texttt{.zip} files as input, and it can be added as a step on top of any existing \texttt{Stream} handling those other integrations. 

Yet another issue \texttt{Stream} helps to solve is the "snapshot problem": where traditional APIs only access the state of data at a single point in time. By having helper functions that easily allow for refreshing, it emulates the experience of having a constant stream of notifications about changes to data. (While simple in concept, constant refresh is another area where non-\texttt{Stream} style APIs often run into the complex error and authentication handling issues from above.) Having to manually handle constant refreshing with updated data is a major complication in a world where almost all APIs are built to be used once per interaction, so this removes a major barrier.

All of these amount to a developer experience that is uniquely well-suited to the problem of keeping cloud data from various sources in sync, hopefully achieving the final goal of building a user-base that enables the ecosystem of integrations.

\subsection{\texttt{fetchReconcileDiff}}
\texttt{fetchReconcileDiff} takes in new data from a source (an API, a data export, etc.) and the currently stored snapshot from that integration. It automatically figures out which items have been updated, fetches any additional information needed for those items, reconciles any differing formats, and compares the new snapshot to the old one to determine changes. It returns both the next stored snapshot and a listing of the changes made. These three processes: fetching, reconciling, and determining the changes are all run simultaneously, since they share many of the same steps, and can be intertwined.\

When data from a source is run through \texttt{fetchReconcileDiff}, it is separated into three types: new items, overlapping items, and stale items (the saved data not present in the new data.) New items have additional information fetched and saved every time. For overlapping items, it compares the new data with the stored data to determine if it's been updated, and only fetches and saves those updates. For stale data, there are simple configuration options to determine what to do. (Have they been deleted, or is the input data just incomplete?).

Typically, the input data passed to \texttt{fetchReconcileDiff} is incomplete in some way. Most API endpoints that list large amounts of data only include basic metadata like identifiers, which can be passed to more specific API calls to get more detail. (The same goes for data exports.) This is usually a problem--the "resource problem"--because making potentially hundreds of API calls to access all the details is enormously inefficient. But because \texttt{fetchReconcileDiff} usually knows which items have been edited since the last snapshot (by comparing metadata like a "last modified" timestamp), it only has to make those detailed requests for the portions of the data that have changed. It also has an optional cache system, so that if different items have some subset of their data that is shared, it only has to be fetched once. All of the logic for identifying which items have been changed and how to fetch more details are easily customizable by the developer, but are segmented so that the configuration is still very simple to understand.

To mark which parts of the data need to be updated, \texttt{fetchReconcileDiff} associates every item in the new data with its counterpart in the currently stored data if it exists. Having both the new and stored versions of an item during the fetch process allows not only for certain fetches to be cached, preventing redundant requests, but also for data from either version to be filled in to the final combined item, reconciling them together.

This reconciliation pattern also helps with temporal reconciliation. A big part of the snapshot problem is the fact that snapshots like data exports can be asynchronous, and thus any given snapshot might be out of date by the time it's processed. By reconciling with the existing data rather than replacing it, there isn't the risk of losing new data. 

\texttt{fetchReconcileDiff} returns a data structure that includes both a snapshot of the newly updated source of truth, and more detailed information delineating what has been added, changed, or removed since the previous snapshot, based on the separated types of data it's already done. These details help reconstruct the specific changes to the data, rather than just the current state, making the "events" half of the "event stream" a reality.

\subsection{Other design choices}
One major design choice made early in the process is that Aqueduct is designed primarily to run in server environments rather than browser ones (though there are parts of it that can be run in the browser). The main reason for this is simple: due to cross-origin resource sharing (CORS) restrictions, it is often impossible to fetch data from a 3rd party server inside a webpage with a different domain.~\cite{mozillaReasonCORSHeader2025} For example, it's not possible to access Notion data in-browser from any domain except \texttt{notion.so}, but it is possible from a separately-running server. 

The original vision was to have everything runnable in-browser, to promote the idea of a client where users had all the data on-device, rather than it being stored on yet another server. But Aqueduct integrations should be able to be used by anyone, on any project, and most of those projects involve some central server. Even local-first projects like my original vision have increasingly been moving towards centralized servers that partially sync data to clients, but offload compute and storage.~\cite{rocicorpZero} And really, there is too much data being stored from these cloud services to have a copy on every device. Lastly, integrations running on servers allows them to access scraping libraries, which are key for getting data that isn't accessible through traditional APIs or exports (or even for automating that export process.)

Instead of enforcing that Aqueduct run on users' devices, as part of the demo application, I began building a self-hosted server that would live on a user's computer, handle most of the non-platform-specific syncing, and be the central store. Then, connected devices can only download the data relevant to them, and upload any data from the integrations specific to their platform. The server isn't a part of this thesis's scope (other than assisting with the demo application), but likely will be an important part of the future of the Aqueduct ecosystem. However, Aqueduct itself is storage-agnostic, again to make it as widely useful as possible.

Aqueduct is written in TypeScript, an interoperable version of JavaScript that encourages static typing of variables. This didn't (and doesn't) have to be the case. Aqueduct is primarily a design specification rather than a heavy implementation, and could easily be ported to other languages and platforms (more on that in the Future Work section). However, I deliberately chose TypeScript for a couple of reasons. First, it and JavaScript combine to be the 2nd most popular programming language in the world, and easily the most popular for user-facing software (the #1, Python, dominates in the machine learning arena).~\cite{cassTopProgrammingLanguages2024} This makes the population of developers who could use and build integrations with Aqueduct as large as possible. Moreover, web APIs and data exports almost always return their data in JSON format, which maps directly to the object format of JavaScript and TypeScript. (In fact, JSON stands for JavaScript Object Notation). This makes the language incredibly well-suited for working with these payloads. Finally, through projects like React Native, TypeScript can run on almost any device, albeit with worse performance than truly native code. 

Finally, the API of the integrations themselves.  (Confusingly, the term "API" is used to describe both the web APIs that I'm interacting with over the network, and the design of code interfaces exposed by libraries like Aqueduct.) These integrations are simply collections of idempotent functions, along with some immutable global state shared among those functions (like client IDs for APIs, which are hardcoded values attached to every request, or SDK helpers for constructing the web requests).

I experimented with forcing more structure upon the integrations (especially at the start of the thesis), but the wide variety of necessary functions among different integrations made imposing one ontology impossible. Additionally, much of the sequencing I was trying to force is inherent in idempotent functions. With no side effects, it's clear an authentication function returning an access token needs to be run before a data request function that takes that access token as a parameter, for example.

A benefit of this flexibility developers building integrations can use whatever other libraries inside of them they want, such as scraping libraries or official SDKs. Because there are no restrictions other than idempotency, it's no more challenging to build an entire integration than it is to write a small script making a single snapshot request. In fact, the design goal for building integrations in Aqueduct is that a developer should be able to make a single request in a test script, then wrap that exact code in an Aqueduct integration and have it work with as few changes as possible.

There are some ideas I want to explore in the future: adding sections for flows that combine commonly-chained steps into one function, and sections for functions that can be run only on-server, only in-browser, or on both platforms. But these, too, would be design recommendations. At the end of the day, giving developers the flexibility to compose functions as fits their application is the most important factor. 

\section{Discussion}
Aqueduct shows that even with the limited, snapshot-style data access options available today, it's possible to use them in a much more modern, reactive event-driven style. Even differentiating between these two types creates a distinction that is helpful towards understanding how to improve our current methods of data access.

While it isn't yet ready for release, when it does, it will allow developers to get a stream of events of the data from a cloud application with little more work than making a single request to its API (or less, if they build on top of a pre-built integration). In turn, developers can create applications that act as alternative clients for cloud apps, operating off of almost full-featured, real-time data. (As discussed, getting that full-featured data is a barrier that is currently almost impossibly high.)

The biggest limitation of this project is that it is, at some level, all theory. Though I iterated on and validated many of the design decisions through building the corresponding demo application, it remains to be seen if other developers see Aqueduct as an improvement over their current interactions with APIs. Hopefully over time, this ecosystem will grow to allow developers to integrate with almost all of the cloud applications users currently have their data siloed in.

\subsection{Future work}
Most of the future work on Aqueduct revolves around polishing it up in order to release it as a library anyone can use. It currently has some redundant files I used for explorations, some code that's inefficient, and it's lacking documentation on certain elements of its API. I also want to move integrations from one central hub inside the \texttt{aqueduct} package to be their own individual projects, so it's possible to pick and choose which ones to use. 

In terms of making the code more efficient, as well as improving the developer experience in general, it would be helpful to think about a more advanced diffing helper. Right now, scaffolding is set up to automate diffing at the item level, but within items, it's up to the developer to differentiate the changes that have occurred. Adding a version of the JSON patch API~\cite{caoJSONPatchTurning2016}, or something similar that easily supports Aqueduct's reconciliation needs would further improve the developer experience.

As illustrated in Overeem et al., another issue future versions of Aqueduct will have to tackle is the issue of data sources changing schema over time.~\cite{overeemDarkSideEvent2017} A flexible reconciliation protocol would ideally go a long way towards being flexible towards schema changes as well, but schema migration is one of the most challenging problems in application design, so will require serious effort.

Longer-term, Aqueduct will be most successful if it comes alongside a pre-built server that handles syncing data from various sources and exposes it to client applications. That server, called Reservoir, is something I plan to work on once Aqueduct has been finalized for its initial release. (Truly long-term, an application that displays this aggregation of users' data to them and lets them interact with it will drive adoption even further. The demo application in the \texttt{aqueduct} repository, Fountain, is an extremely early prototype of this.)

Also in the long-term, I'd like Aqueduct to be available on more platforms than just JavaScript ones. Since the API of the library is much more important than its current implementation, it could be easily ported into a more cross-platform language in the future.

\section{Acknowledgments}
I'd like to thank my advisor, Jeff Huang, for his guidance, advice, and many insights throughout the thesis process, as well as my second reader, Nikos Vasilakis, for his advice and feedback on my work. This is the culmination of all the learning and thinking I've done at Brown about computers, so I'd also like to thank all the professors that have taught me along that journey. Finally, to my parents: thank you for letting me talk to you endlessly about this project, and for being unconditionally supportive of my love of technology throughout my life--I wouldn't be here without you.

\bibliographystyle{ACM-Reference-Format}
\bibliography{thesis}

\appendix
\section{The Aqueduct System}
Though the general \texttt{Stream}/\texttt{fetchReconcileDiff} combination addresses the core principles of my research, the specific design choices of details within each of those components, as well as the other helpers, all are important for creating a great ecosystem of integrations. 

\subsection{`Stream'}
First up is \texttt{Stream}, which has been described at a higher level
above. In my implementation, this is a wrapper around the
\texttt{xstream} library for JavaScript, but it could be reasonably
converted to any of the stream libraries in most major programming
languages. (It\textquotesingle s even possible to create a custom stream
implementation. Originally, Aqueduct had a custom, from-scratch
\texttt{Stream} class, but moved to the \texttt{xstream} wrapper for
robustness.) Instead, the design of the API for using
\texttt{Stream}-\/-the methods available on it, its initializers, and
its returns-\/-and that API\textquotesingle s suitability for syncing
data is the novel area.

Each step in the chain of a \texttt{Stream} runs every time the previous
step emits any value, and can emit as many or few values of their own as
they want (including none).

A \texttt{Stream} can be created with a number of initializers. Some of
them are basic, like \texttt{Stream.from({[}1,\ 2,\ 3{]})} and
\texttt{Stream.of(1,\ 2,\ 3)}, which emit all of their values
essentially instantaneously, and are mainly used for testing purposes.
Others are more specific to the task. The data that initializes a
\texttt{Stream} is usually the first data in the process; often
something like an authentication token. That data has to get to the
server somehow, and there are two initializers that are specifically
designed for the task of starting integrations on a server:

\begin{enumerate}
\tightlist
\item
  \texttt{Stream.fromListener} is used when the initializing data is
  stored in something that can be listened to (e.g a database with
  subscription functionality). Then, a stream can be started using, for
  example,
  \texttt{Stream.fromListener(emit\ =\textgreater{}\ db.subscribe(data\ =\textgreater{}\ emit(data))}.
\item
  \texttt{Stream.fromHandle} is used if the initializing data is a
  one-off communication (e.g. if the server handles HTTP PUSH requests
  from the client). A stream can be started using
  \texttt{const\ {[}handle,\ stream{]}\ =\ Stream.fromHandle\textless{}Type\textgreater{}()},
  and later begun by running \texttt{handle(data)}. Crucially, this
  initializer returns both the \texttt{Stream} itself to be built on top
  of, and the callback handle that can be returned from the function
  that sets up the entire stream, and then passed around to whatever
  locations need to call it and begin the stream. There is another
  function, \texttt{Stream.create}, that is the generic initializer
  provided by the \texttt{xstream} library, that these are built on top
  of, and allows for finer-grained control.
\end{enumerate}

Once a \texttt{Stream} has been initialized, there are a number of
transformation functions that can be run on it. Each returns another
\texttt{Stream}, just with another step added to it. Again, many of
these such as \texttt{stream.take(n:\ number)} and
\texttt{stream.drop(n:\ number)} are trivial. There are many, though,
that are essential for the ergonomics of chaining together functions
from integrations.

\begin{itemize}
\tightlist
\item
  \texttt{stream.map}, like the \texttt{map} function on lists in many
  standard libraries, transforms data from one type into another using a
  given mapping function. There are some important nuances in
  Aqueduct\textquotesingle s implementation, however:

  \begin{itemize}
  \tightlist
  \item
    Foremost is that rather than having a separate \texttt{mapAsync}
    function, \texttt{map} accepts both synchronous and asynchronous
    transformation functions and treats them equally. Most sync
    functions in the integrations require some kind of asynchronous web
    access, so it makes sense to treat them as a first-class citizen.
    Simultaneously, many of these supposedly asynchronous functions
    won\textquotesingle t actually be that way due to the caching
    capabilities of \texttt{fetchReconcileDiff}, so it further makes
    sense to treat them interchangeably.
  \item
    Additionally, it accepts a parameter for the number of parallel
    functions that can be running. If an asynchronous function takes
    longer to run than the preceding function emits events, requests
    will be duplicated and unnecessary resources will be used, which
    this parameter prevents.
  \end{itemize}
\item
  \texttt{stream.filter} follows a similar structure, but replaces the
  \texttt{filter} function. Notably, this means that not every event
  from a source stream will get emitted farther down the chain.
\item
  \texttt{stream.dropRepeats} is a key for limiting resource
  consumption. Because Aqueduct can work with any storage framework,
  it\textquotesingle s hard to tell exactly how many times the
  \texttt{Stream}\textquotesingle s initializer will emit. (In the
  example with the subscription-capable database, it might notify
  subscribers any time any data is changed, even if that data
  isn\textquotesingle t the exact subset that\textquotesingle s being
  observed.) Preventing repeats prevents redundant requests that chew up
  rate limits.
\item
  \texttt{stream.onEach} is a version of \texttt{map} that runs a given
  function on every emit, but always returns the input value.
  It\textquotesingle s useful for two things: logging and saving things
  outside of the \texttt{Stream}:

  \begin{itemize}
  \tightlist
  \item
    Adding a \texttt{console.log} that prints on each event is essential
    while debugging if something in the chain is going wrong.
  \item
    If a developer wants to keep an external source up to date about
    things going on inside the chain, they can do it in an
    \texttt{onEach}. For example, a developer could add an
    \texttt{onEach} step after a login token has been exchanged, but
    before it\textquotesingle s been used for a web request, that saves
    it to a database (so that if the server were to restart, the user
    wouldn\textquotesingle t have to log back in).
  \end{itemize}
\item
  \texttt{stream.every} is the function that enables the automatic
  refresh functionality. It takes in an interval to refresh upon, as
  well as optional functions to save the timestamp of the emit to a
  database and load that timestamp back (so a server that restarted in
  the middle of a 10 hour refresh process won\textquotesingle t cut the
  wait time short). Then it emits its most recent input value upon each
  interval, triggering the sync functions downstream.
\item
  \texttt{stream.combine} takes the outputs of two streams and any time
  one emits, it emits with the latest values of both streams (provided
  both have emitted at least once). This is key for bringing together
  different data sources (e.g. an authentication token that is passed
  from an HTTP handler the first time, but retrieved from a database in
  subsequent initializations). Since Aqueduct is agnostic to the storage
  and servers it\textquotesingle s being run on, these functions can be
  combined in any way that is needed to support that specific
  architecture, the integrations the developer wants to use, and more.
\end{itemize}

The final component of the \texttt{Stream} process is the listener,
which contains a callback that runs on every emitted value, but
doesn\textquotesingle t emit any of its own. In order for an initializer
to begin running, a listener \emph{must} be attached.
It\textquotesingle s called much like the transformation functions, but
returns \texttt{void} instead of a \texttt{Stream}. It takes a function
much like the one passed to \texttt{onEach}, plus two additional
optional functions \texttt{onError} and \texttt{onComplete} (which is
called if one of the basic initializers, like
\texttt{Stream.of(1,\ 2,\ 3)}, runs out of values).

Why is it necessary to attach a listener to initialize rather than just
having any created \texttt{Stream} automatically initialize and run
(replacing the listener with a simple \texttt{onEach})? The basic
problem is this: the initializer might start emitting values before all
the steps are attached. In a simplified representation, each step of the
\texttt{Stream} essentially has a handle for passing it values, and a
slot that takes in another stream\textquotesingle s handle, which it
calls when it needs to emit values to. Any time a step is added to a
\texttt{Stream}, it sets the slot of the previous step to its handle.
But if at any point that slot is empty, the value just
isn\textquotesingle t emitted. There are a number of ways to fix this
(all of which I tried):

\begin{itemize}
\tightlist
\item
  One option is to delay every initializer by some number of
  milliseconds, in order for all of the steps to get attached. This
  quickly becomes a race condition, however; what if a really slow
  computer or an asynchronous attachment took more than the arbitrary
  delay?
\item
  Another option is to store the most recently emitted value alongside
  the slot, and anytime the slot gets filled, the most recent value gets
  passed to it. In addition to adding a lot of complexity to the
  internal workings, this also creates a much more subtle race
  condition. Imagine a hypothetical function
  \texttt{Stream.delayedBy({[}1,\ 2,\ 3{]},\ milliseconds(2))}. By only
  storing the most recent value, depending on the speed of attachment,
  subsequent steps might get all of the emissions or only some recent
  subset. (Storing and replaying the results of every emission is, of
  course, extremely impractical.)
\item
  The final, and best option, is to only run the initializer once a
  listener has been attached, marking that there will be no more steps
  attached on top of that. Only then (which, to be clear, is usually
  less than a millisecond later) can the initializer be sure that every
  step will get every value. (This still has a slight issue where if the
  handle from \texttt{Stream.fromHandle} somehow gets returned and
  called before a listener is attached, it won\textquotesingle t emit,
  but this is an edge case that should be obviously wrong.)
\end{itemize}

\subsection{`fetchReconcileDiff'}
Similar to \texttt{Stream}, \texttt{fetchReconcileDiff} has been
described above at a high level, but has a number of interesting
details.

The key to understanding \texttt{fetchReconcileDiff} is understanding
its types. \texttt{fetchReconcileDiff} is a generic function that takes
in 5 generic types. Any possible type can be assigned to each of those
generic types, but those types are then enforced for that instance of
\texttt{fetchReconcileDiff}. (In real-world usage, these types usually
don\textquotesingle t have to be set explicitly-\/-TypeScript infers
them from the context of the other code.)

These generic types are:

\begin{enumerate}
\tightlist
\item
  \texttt{Current} - the type of the data being passed into the
  function, which is usually incomplete or different from the
  already-saved data in some way.
\item
  \texttt{Stored} - the type of the data that will be output from the
  function, as well as the type of the previously stored data that is
  passed into the function as the previous snapshot.
\item
  \texttt{Identifier} - the type of the subset of the input data that
  uniquely identifies a certain item. It defaults to a \texttt{string}.
\item
  \texttt{Signature} - the type of the subset of the input data that
  uniquely identifies a certain \emph{version} of an item. If an item
  has changed between snapshots, the \texttt{Identifier} should be the
  same, but the \texttt{Signature} should be different.

  \begin{itemize}
  \tightlist
  \item
    As an example, Spotify playlists include a value called
    \texttt{snapshot\_id} that changes any time a change is made to a
    playlist. If the \texttt{snapshot\_id} of the \texttt{Current} and
    \texttt{Stored} versions of the playlist are the same,
    they\textquotesingle d be identical at the end of the fetch and
    convert process.
  \end{itemize}
\item
  \texttt{Cache} - the type of the cache holding relevant stored
  information that might be shared amongst previously stored and truly
  new items. Defaults to a \texttt{Map} from \texttt{Signature} to
  \texttt{Stored}.
\end{enumerate}

With these types in mind, the syntax of \texttt{fetchReconcileDiff} can
be explored. \texttt{fetchReconcileDiff} is a single, asynchronous
function that takes in six required parameters, along with a number of
optional ones. The six required are as follows:

\begin{enumerate}
\tightlist
\item
  \texttt{currentItems:\ Current{[}{]}} The list of items that will be
  fetched, converted, and diffed.

  \begin{itemize}
  \tightlist
  \item
    (At some point, this will accept non-lists as well, but in practice
    so far almost every single source returns what is essentially a
    list.)
  \end{itemize}
\item
  \texttt{storedItems:\ Stored{[}{]}} The list of currently stored items

  \begin{itemize}
  \tightlist
  \item
    (Again, at some point this could take in other types including
    \texttt{Map}s or even database connections which might be more
    memory-efficient, but which would provide essentially the same
    abstraction.)
  \end{itemize}
\item
  \texttt{currentIdentifier:\ (currentItem:\ Current)\ =\textgreater{}\ Identifier}
  A function that maps a \texttt{Current} item to the value that
  identifies it uniquely among other \texttt{Current} items
\item
  \texttt{storedIdentifier:\ (storedItem:\ Stored)\ =\textgreater{}\ Identifier}
  A function that maps a \texttt{Stored} item to the value that
  identifies it uniquely among other \texttt{Stored} items. For
  \texttt{Current} and \texttt{Stored} items to be properly associated
  with each other, their identifiers must be equal.
\item
  \texttt{convert:\ fetchReconcileDiffConvert\textless{}Current,\ Stored,\ Cache\textgreater{}}
  A function that transforms \texttt{Current} items into \texttt{Stored}
  ones, often by fetching additional information from an API,
  reconciling with the corresponding \texttt{Stored} item, or getting
  cached data from other stored items.

  \begin{itemize}
  \tightlist
  \item
    \texttt{fetchReconcileDiffConvert} is a union type that allows for
    two different kinds of conversions, \texttt{each} or \texttt{all}

    \begin{itemize}
    \tightlist
    \item
      \texttt{each:\ (currentChangedItem:\ Current,\ cache:\ Cache,\ correspondingStoredItem?:\ Stored)\ =\textgreater{}\ Stored\ \textbar{}\ Promise\textless{}Stored\textgreater{}}
      has the developer provide a function that runs on each
      \texttt{Current} item and converts it to its \texttt{Stored}
      format with the help of the \texttt{Cache} and its corresponding
      \texttt{Stored} item from the source of truth, if it exists.
    \item
      \texttt{all:\ (currentChangedItems:\ Current{[}{]},\ cache:\ Cache,\ correspondingStoredItems:\ (Stored\ \textbar{}\ undefined){[}{]})\ =\textgreater{}\ Stored{[}{]}\ \textbar{}\ Promise\textless{}Stored{[}{]}\textgreater{}}
      is the same, but the function provides the entire list to the
      developer to convert all at once. This is useful, for example, to
      batch data requests if an API lets you get detailed information
      about multiple items at once. (Later on, the
      \texttt{fetchWindowed} helper will explain this in more detail.)
    \end{itemize}
  \item
    Notably, synchronous or asynchronous functions can be used
    interchangeably, for the same reason as in \texttt{Stream.map}
  \end{itemize}
\item
  \texttt{keepStaleItems:\ boolean\ \textbar{}\ ((staleItems:\ Stored{[}{]},\ nonStaleItems:\ Stored{[}{]})\ =\textgreater{}\ Stored{[}{]})}
  A value or function that determines if stale items (ones that
  weren\textquotesingle t included in the current list) are kept or
  removed.

  \begin{itemize}
  \tightlist
  \item
    In some cases, missing items in new data doesn\textquotesingle t
    mean stale data should be removed. For instance,
    Spotify\textquotesingle s Recent Listens endpoint only returns the
    50 most recent tracks, which shouldn\textquotesingle t overwrite all
    of the previous ones, so \texttt{keepStaleItems} should be true.
  \item
    Other times, the opposite is true. If Spotify\textquotesingle s
    Playlists endpoint is missing a playlist that previously was saved,
    it means that playlist has been deleted, so \texttt{keepStaleItems}
    should be false
  \item
    Finally, there are some times where some stale items should be kept,
    and some removed. For those situations, \texttt{keepStaleItems} also
    accepts a function that filters down which should be kept. One
    example of when this might be necessary is discrepancies between
    different sources in the same integration. If a Spotify user is
    playing a song, and scrubs to a new position in the track, the
    Currently Playing endpoint counts it as a brand new "listen". But
    Spotify\textquotesingle s Data Export, the more official source of
    truth, counts it only as one listen. If listens have been saved from
    Currently Playing, and now are being imported from an export, any
    stale listens between the earliest and latest listen of the export
    should be removed. But since the export takes up to a month to
    create, Currently Playing will often have picked up new listens
    after the latest listen, and those, while "stale", are just due to
    incomplete data, and should be kept. By calculating the time range
    of \texttt{nonStaleItems}, and keeping only the \texttt{staleItems}
    outside that time range, this is possible without leaving the
    \texttt{fetchReconcileDiff} framework.
  \end{itemize}
\end{enumerate}

There are some other optional parameters to \texttt{fetchReconcileDiff}
for more complicated workflows:

\begin{itemize}
\tightlist
\item
  \texttt{currentSignature} and \texttt{storedSignature} are the same as
  \texttt{currentIdentifier} and \texttt{storedIdentifier}, but for the
  \texttt{Signature} that determines if items are the same version. For
  example, in any API that returns a timestamp for when an item was last
  modified, the signature can simply be the item\textquotesingle s
  identifier plus its last modified timestamp.
\item
  \texttt{createCache} allows the developer to create a custom cache of
  information to be checked before fetching new data from an API, which
  is available in \texttt{convert}. If the developer
  doesn\textquotesingle t specify a custom one, it defaults to a
  \texttt{Map} from each item\textquotesingle s \texttt{Signature} to
  that item.
\end{itemize}

Finally there are two helper functions for the actual data fetching
process, which might be called inside \texttt{convert} or to get the
data passed to \texttt{currentItems}:

\begin{itemize}
\tightlist
\item
  \texttt{fetchPaged} is for situations where a developer is getting the
  initial list of items from an API, but it has a maximum number of
  results it returns per request, and to get more, the developer has to
  request subsequent "pages" of items. This helper automates requesting
  all of those pages, so they can be treated like one single request
  that returns all of the items.
\item
  \texttt{fetchWindowed} is for situations where a developer has already
  gotten a list of simple items, but needs to get more detail for each,
  \emph{and} the API has a mechanism to request details for multiple ids
  at once (with some maximum batch size). \texttt{fetchWindowed}
  automatically batches the simple data to a certain maximum size, then
  runs a single request for each batch.
\end{itemize}

The internal flow of \texttt{fetchReconcileDiff} consists of five main
steps:

\begin{enumerate}
\tightlist
\item
  Assign an \texttt{Identifier} and a \texttt{Signature} to each
  \texttt{Current} item and \texttt{Stored} item, and associate them if
  they\textquotesingle re the same
\item
  Use those values to classify each item into one of four buckets: new,
  updated, unchanged, or stale.
\item
  Create the cache with \texttt{createCache}
\item
  Use the cache and associated items to run \texttt{convert}
\item
  Based on the original buckets (and with the stale items split into
  stale and removed items by \texttt{keepStaleItems}), return the
  result, consisting of each bucket and a combination of all kept items.
\end{enumerate}

\subsection{Other helpers}
There are a couple other helper functions in Aqueduct that make it
easier to deal with common tasks in the sync workflow:

\begin{itemize}
\tightlist
\item
  \texttt{generateUUID} is a function that creates a new UUID (useful
  for assigning an identifier to data that comes without one) and works
  on server and browser interchangeably.
\item
  \texttt{unzipFiles} and its little brother, \texttt{unzipFile}, are
  for handling data exports, most of which come in \texttt{.zip} format
\end{itemize}

For those looking to explore Aqueduct in more detail, the library and a
demonstration application is available at
\texttt{https://github.com/andyburris/aqueduct}.

\end{document}
